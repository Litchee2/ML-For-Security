{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先导入URLs列表，看下大体情况..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:37.531845Z",
     "start_time": "2019-08-28T11:07:37.329057Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:37.538884Z",
     "start_time": "2019-08-28T11:07:37.532764Z"
    }
   },
   "outputs": [],
   "source": [
    "malicious_urls = pd.read_csv('test.txt', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:37.550880Z",
     "start_time": "2019-08-28T11:07:37.540190Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>bd-un1.wanglv.com/auycbe.js</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>www.hzed.com/front/activity/verifyMobileForReg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>47.93.177.253:4346/HZJH.aspx?Lottery=CQSSC&amp;Mke...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>115.231.155.32/ttyzcpzoomtmbcbohggmaztaspeannb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>222.73.132.173/zijian.hls.video.qq.com/DE26960...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2157</td>\n",
       "      <td>m.nbflq.com/js/error.js?20180608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2158</td>\n",
       "      <td>sinacloud.net/qunimg/rmsp.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2159</td>\n",
       "      <td>61.147.235.246/dlied1.qq.com/iedsafe/sanlix/1a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>www.qyffw.com/favicon.ico</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2161</td>\n",
       "      <td>211.151.23.149/config</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2162 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  1\n",
       "0                           bd-un1.wanglv.com/auycbe.js  1\n",
       "1        www.hzed.com/front/activity/verifyMobileForReg  1\n",
       "2     47.93.177.253:4346/HZJH.aspx?Lottery=CQSSC&Mke...  1\n",
       "3     115.231.155.32/ttyzcpzoomtmbcbohggmaztaspeannb...  1\n",
       "4     222.73.132.173/zijian.hls.video.qq.com/DE26960...  1\n",
       "...                                                 ... ..\n",
       "2157                   m.nbflq.com/js/error.js?20180608  1\n",
       "2158                      sinacloud.net/qunimg/rmsp.png  1\n",
       "2159  61.147.235.246/dlied1.qq.com/iedsafe/sanlix/1a...  1\n",
       "2160                          www.qyffw.com/favicon.ico  1\n",
       "2161                              211.151.23.149/config  1\n",
       "\n",
       "[2162 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malicious_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再看个黑白样本的数量分布情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:37.555081Z",
     "start_time": "2019-08-28T11:07:37.551905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1794\n",
       "0     368\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malicious_urls[1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "黑样本居多，但还不至于出现样本不均衡问题，所以在模型选择和评估方面，还不需要注意这方面的问题！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们对URL排个序，看看该如何选择特征..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:37.564045Z",
     "start_time": "2019-08-28T11:07:37.555785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1921</td>\n",
       "      <td>1.1/android_update.htm?uid=9DC3091F90310C72FFE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581</td>\n",
       "      <td>1.wenzhangba.cn/bwosfoscy.js</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.wenzhangba.cn/source/js/web/35zptm.js?p=kcgm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>752</td>\n",
       "      <td>112.117.221.11/qpdxv/v0/20160324/7b/7e/86b3cea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2151</td>\n",
       "      <td>112.117.221.11/qpdxv/v0/20181108/d8/ea/348617c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1099</td>\n",
       "      <td>zuikzy.com/?m=vod-detail-id-17332.html</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>886</td>\n",
       "      <td>zuqhxz.byrdyryr.com:43564/ccc.aspx?g=fb28gY+hQ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>zxcs.xiaodu9.cn/favicon.ico</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1267</td>\n",
       "      <td>zxcs1.qlrtr.cn/orders/v2/pay?order_id=KHFZOK59...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>842</td>\n",
       "      <td>zz.amazingfuture.cn/robot/account/pong</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2162 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  1\n",
       "1921  1.1/android_update.htm?uid=9DC3091F90310C72FFE...  1\n",
       "581                        1.wenzhangba.cn/bwosfoscy.js  1\n",
       "580   1.wenzhangba.cn/source/js/web/35zptm.js?p=kcgm...  1\n",
       "752   112.117.221.11/qpdxv/v0/20160324/7b/7e/86b3cea...  1\n",
       "2151  112.117.221.11/qpdxv/v0/20181108/d8/ea/348617c...  1\n",
       "...                                                 ... ..\n",
       "1099             zuikzy.com/?m=vod-detail-id-17332.html  1\n",
       "886   zuqhxz.byrdyryr.com:43564/ccc.aspx?g=fb28gY+hQ...  1\n",
       "320                         zxcs.xiaodu9.cn/favicon.ico  1\n",
       "1267  zxcs1.qlrtr.cn/orders/v2/pay?order_id=KHFZOK59...  1\n",
       "842              zz.amazingfuture.cn/robot/account/pong  1\n",
       "\n",
       "[2162 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malicious_urls.sort_values(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T04:03:35.825476Z",
     "start_time": "2019-06-12T04:03:35.817112Z"
    }
   },
   "source": [
    "大致看了下，该URLs数据集定义恶意URL的标准，并不同于直白的比如基于web攻击类型的定义，而是基本上是涉及诸如黄色图片，视频，赌球，涉政等问题，而对于这种问题的恶意定义都是主观性的，而如果使用特征抽取的传统机器学习方法来学习，特征的寻找相对来说比较广泛，所以想尝试使用深度学习中的类似情感分析的方法来解决这类问题，比如使用conv1d，或rnn，lstm等模型来处理，因这类文本在上下文之间并不需要存在明显的关联性，所以决定采用conv1d，文本无关联需求的1维卷积来处理这类文本，速度上也会快很多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:37.569430Z",
     "start_time": "2019-08-28T11:07:37.564911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             bd-un1.wanglv.com/auycbe.js\n",
       "1          www.hzed.com/front/activity/verifyMobileForReg\n",
       "2       47.93.177.253:4346/HZJH.aspx?Lottery=CQSSC&Mke...\n",
       "3       115.231.155.32/ttyzcpzoomtmbcbohggmaztaspeannb...\n",
       "4       222.73.132.173/zijian.hls.video.qq.com/DE26960...\n",
       "                              ...                        \n",
       "2157                     m.nbflq.com/js/error.js?20180608\n",
       "2158                        sinacloud.net/qunimg/rmsp.png\n",
       "2159    61.147.235.246/dlied1.qq.com/iedsafe/sanlix/1a...\n",
       "2160                            www.qyffw.com/favicon.ico\n",
       "2161                                211.151.23.149/config\n",
       "Name: 0, Length: 2162, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malicious_urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:38.573737Z",
     "start_time": "2019-08-28T11:07:37.570205Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models, utils, preprocessing, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:38.629921Z",
     "start_time": "2019-08-28T11:07:38.575229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7758 unique tokens.\n",
      "Max sequence length 141 .\n"
     ]
    }
   ],
   "source": [
    "max_words = 10000\n",
    "tokenizer = preprocessing.text.Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(malicious_urls[0])\n",
    "sequences = tokenizer.texts_to_sequences(malicious_urls[0])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "maxlen = 0\n",
    "for i in sequences:\n",
    "    if len(i) > maxlen:\n",
    "        maxlen = len(i)\n",
    "print('Max sequence length %s .' % maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:38.636714Z",
     "start_time": "2019-08-28T11:07:38.631072Z"
    }
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "    malicious_urls = pd.read_csv('test.txt', sep='\\t', header=None)\n",
    "    \n",
    "    max_words = 10000\n",
    "    tokenizer = preprocessing.text.Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(malicious_urls[0])\n",
    "    sequences = tokenizer.texts_to_sequences(malicious_urls[0])\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    maxlen = 0\n",
    "    for i in sequences:\n",
    "        if len(i) > maxlen:\n",
    "            maxlen = len(i)\n",
    "\n",
    "    X = preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
    "    y = malicious_urls[1].values\n",
    "    \n",
    "    Xtrain, Xtest, ytrain, ytest = X[:2000], X[2000:], y[:2000], y[2000:]\n",
    "    \n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:38.643876Z",
     "start_time": "2019-08-28T11:07:38.637587Z"
    }
   },
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    callbacks.EarlyStopping(\n",
    "    monitor='acc',\n",
    "    patience=1,),\n",
    "    callbacks.ModelCheckpoint(\n",
    "    filepath='my_model1.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,)] \n",
    "\n",
    "def create_model(Xtrain, ytrain, Xtest, ytest):\n",
    "    malicious_urls = pd.read_csv('test.txt', sep='\\t', header=None)\n",
    "    \n",
    "    max_words = 10000\n",
    "    tokenizer = preprocessing.text.Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(malicious_urls[0])\n",
    "    sequences = tokenizer.texts_to_sequences(malicious_urls[0])\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    maxlen = 0\n",
    "    for i in sequences:\n",
    "        if len(i) > maxlen:\n",
    "            maxlen = len(i)\n",
    "            \n",
    "    callbacks_list = [\n",
    "        callbacks.EarlyStopping(\n",
    "        monitor='acc',\n",
    "        patience=1,),\n",
    "        callbacks.ModelCheckpoint(\n",
    "        filepath='my_model1.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,)] \n",
    "            \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Embedding(len(word_index) + 1, {{choice([64, 128])}}, input_length=maxlen))\n",
    "    model.add(layers.Conv1D({{choice([64, 128])}}, {{choice([5, 7, 9])}}, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer={{choice(['rmsprop', 'adam', 'nadam'])}}, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    result = model.fit(Xtrain, ytrain, epochs={{choice([10, 20, 50, 100])}}, batch_size={{choice([50, 100])}}, validation_split=0.1, callbacks=callbacks_list)\n",
    "    \n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:48.496090Z",
     "start_time": "2019-08-28T11:07:38.644684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import layers, models, utils, preprocessing, callbacks\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Conv1D': hp.choice('Conv1D', [64, 128]),\n",
      "        'Conv1D_1': hp.choice('Conv1D_1', [64, 128]),\n",
      "        'Conv1D_2': hp.choice('Conv1D_2', [5, 7, 9]),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'nadam']),\n",
      "        'epochs': hp.choice('epochs', [10, 20, 50, 100]),\n",
      "        'batch_size': hp.choice('batch_size', [50, 100]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: malicious_urls = pd.read_csv('test.txt', sep='\\t', header=None)\n",
      "  3: \n",
      "  4: max_words = 10000\n",
      "  5: tokenizer = preprocessing.text.Tokenizer(num_words=max_words)\n",
      "  6: tokenizer.fit_on_texts(malicious_urls[0])\n",
      "  7: sequences = tokenizer.texts_to_sequences(malicious_urls[0])\n",
      "  8: word_index = tokenizer.word_index\n",
      "  9: \n",
      " 10: maxlen = 0\n",
      " 11: for i in sequences:\n",
      " 12:     if len(i) > maxlen:\n",
      " 13:         maxlen = len(i)\n",
      " 14: \n",
      " 15: X = preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
      " 16: y = malicious_urls[1].values\n",
      " 17: \n",
      " 18: Xtrain, Xtest, ytrain, ytest = X[:2000], X[2000:], y[:2000], y[2000:]\n",
      " 19: \n",
      " 20: \n",
      " 21: \n",
      " 22: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     malicious_urls = pd.read_csv('test.txt', sep='\\t', header=None)\n",
      "   4:     \n",
      "   5:     max_words = 10000\n",
      "   6:     tokenizer = preprocessing.text.Tokenizer(num_words=max_words)\n",
      "   7:     tokenizer.fit_on_texts(malicious_urls[0])\n",
      "   8:     sequences = tokenizer.texts_to_sequences(malicious_urls[0])\n",
      "   9:     word_index = tokenizer.word_index\n",
      "  10: \n",
      "  11:     maxlen = 0\n",
      "  12:     for i in sequences:\n",
      "  13:         if len(i) > maxlen:\n",
      "  14:             maxlen = len(i)\n",
      "  15:             \n",
      "  16:     callbacks_list = [\n",
      "  17:         callbacks.EarlyStopping(\n",
      "  18:         monitor='acc',\n",
      "  19:         patience=1,),\n",
      "  20:         callbacks.ModelCheckpoint(\n",
      "  21:         filepath='my_model1.h5',\n",
      "  22:         monitor='val_loss',\n",
      "  23:         save_best_only=True,)] \n",
      "  24:             \n",
      "  25:     model = models.Sequential()\n",
      "  26:     model.add(layers.Embedding(len(word_index) + 1, space['Conv1D'], input_length=maxlen))\n",
      "  27:     model.add(layers.Conv1D(space['Conv1D_1'], space['Conv1D_2'], activation='relu'))\n",
      "  28:     model.add(layers.GlobalMaxPooling1D())\n",
      "  29:     model.add(layers.Dense(1, activation='sigmoid'))\n",
      "  30: \n",
      "  31:     model.compile(optimizer=space['optimizer'], loss='binary_crossentropy', metrics=['accuracy'])\n",
      "  32:     result = model.fit(Xtrain, ytrain, epochs=space['epochs'], batch_size=space['batch_size'], validation_split=0.1, callbacks=callbacks_list)\n",
      "  33:     \n",
      "  34:     validation_acc = np.amax(result.history['val_acc']) \n",
      "  35:     print('Best validation acc of epoch:', validation_acc)\n",
      "  36:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
      "  37: \n",
      "Train on 1800 samples, validate on 200 samples     \n",
      "Epoch 1/20                                         \n",
      "  50/1800 [..............................]         \n",
      " - ETA: 1:02 - loss: 0.7155 - acc: 0.1600          \n",
      "                                                   \n",
      " 700/1800 [==========>...................]         \n",
      " - ETA: 2s - loss: 0.5071 - acc: 0.7629            \n",
      "                                                   \n",
      "1400/1800 [======================>.......]         \n",
      " - ETA: 0s - loss: 0.3985 - acc: 0.8193            \n",
      "                                                   \n",
      "1800/1800 [==============================]         \n",
      " - 2s 1ms/step - loss: 0.3428 - acc: 0.8467 - val_loss: 0.0834 - val_acc: 0.9800\n",
      "\n",
      "Epoch 2/20                                         \n",
      "  50/1800 [..............................]         \n",
      " - ETA: 0s - loss: 0.0647 - acc: 1.0000            \n",
      "                                                   \n",
      " 750/1800 [===========>..................]         \n",
      " - ETA: 0s - loss: 0.0383 - acc: 0.9933            \n",
      "                                                   \n",
      "1400/1800 [======================>.......]         \n",
      " - ETA: 0s - loss: 0.0288 - acc: 0.9950            \n",
      "                                                   \n",
      "1800/1800 [==============================]         \n",
      " - 0s 78us/step - loss: 0.0272 - acc: 0.9939 - val_loss: 0.0161 - val_acc: 0.9950\n",
      "\n",
      "Epoch 3/20                                         \n",
      "  50/1800 [..............................]         \n",
      " - ETA: 0s - loss: 0.0029 - acc: 1.0000            \n",
      "                                                   \n",
      " 800/1800 [============>.................]         \n",
      " - ETA: 0s - loss: 0.0072 - acc: 0.9988            \n",
      "                                                   \n",
      "1500/1800 [========================>.....]         \n",
      " - ETA: 0s - loss: 0.0051 - acc: 0.9993            \n",
      "                                                   \n",
      "1800/1800 [==============================]         \n",
      " - 0s 75us/step - loss: 0.0049 - acc: 0.9994 - val_loss: 0.0195 - val_acc: 0.9850\n",
      "\n",
      "Epoch 4/20                                         \n",
      "  50/1800 [..............................]         \n",
      " - ETA: 0s - loss: 6.4270e-04 - acc: 1.0000        \n",
      "                                                   \n",
      " 850/1800 [=============>................]         \n",
      " - ETA: 0s - loss: 0.0016 - acc: 1.0000            \n",
      "                                                   \n",
      "1550/1800 [========================>.....]         \n",
      " - ETA: 0s - loss: 0.0019 - acc: 0.9994            \n",
      "                                                   \n",
      "1800/1800 [==============================]         \n",
      " - 0s 74us/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0225 - val_acc: 0.9850\n",
      "\n",
      "Best validation acc of epoch:                      \n",
      "0.9950000047683716                                 \n",
      "Train on 1800 samples, validate on 200 samples                               \n",
      "Epoch 1/50                                                                   \n",
      " 100/1800 [>.............................]                                   \n",
      " - ETA: 2s - loss: 0.7092 - acc: 0.2100                                      \n",
      "                                                                             \n",
      "1800/1800 [==============================]                                   \n",
      " - 0s 121us/step - loss: 0.5107 - acc: 0.7617 - val_loss: 0.2127 - val_acc: 1.0000\n",
      "\n",
      "Epoch 2/50                                                                   \n",
      " 100/1800 [>.............................]                                   \n",
      " - ETA: 0s - loss: 0.3666 - acc: 0.8400                                      \n",
      "                                                                             \n",
      "1800/1800 [==============================]                                   \n",
      " - 0s 26us/step - loss: 0.3014 - acc: 0.8683 - val_loss: 0.1097 - val_acc: 1.0000\n",
      "\n",
      "Epoch 3/50                                                                   \n",
      " 100/1800 [>.............................]                                   \n",
      " - ETA: 0s - loss: 0.2234 - acc: 0.9000                                      \n",
      "                                                                             \n",
      "1800/1800 [==============================]                                   \n",
      " - 0s 28us/step - loss: 0.1403 - acc: 0.9711 - val_loss: 0.0816 - val_acc: 0.9950\n",
      "\n",
      "Epoch 4/50                                                                   \n",
      " 100/1800 [>.............................]                                   \n",
      " - ETA: 0s - loss: 0.1117 - acc: 0.9900                                      \n",
      "                                                                             \n",
      "1800/1800 [==============================]                                   \n",
      " - 0s 25us/step - loss: 0.0613 - acc: 0.9906 - val_loss: 0.0483 - val_acc: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50                                                                   \n",
      " 100/1800 [>.............................]                                   \n",
      " - ETA: 0s - loss: 0.0354 - acc: 1.0000                                      \n",
      "                                                                             \n",
      "1800/1800 [==============================]                                   \n",
      " - 0s 26us/step - loss: 0.0287 - acc: 0.9978 - val_loss: 0.0335 - val_acc: 0.9950\n",
      "\n",
      "Epoch 6/50                                                                   \n",
      " 100/1800 [>.............................]                                   \n",
      " - ETA: 0s - loss: 0.0122 - acc: 1.0000                                      \n",
      "                                                                             \n",
      "1800/1800 [==============================]                                   \n",
      " - 0s 25us/step - loss: 0.0146 - acc: 0.9983 - val_loss: 0.0191 - val_acc: 1.0000\n",
      "\n",
      "Epoch 7/50                                                                   \n",
      " 100/1800 [>.............................]                                   \n",
      " - ETA: 0s - loss: 0.0180 - acc: 0.9900                                      \n",
      "                                                                             \n",
      "1800/1800 [==============================]                                   \n",
      " - 0s 25us/step - loss: 0.0076 - acc: 0.9989 - val_loss: 0.0265 - val_acc: 0.9850\n",
      "\n",
      "Epoch 8/50                                                                   \n",
      " 100/1800 [>.............................]                                   \n",
      " - ETA: 0s - loss: 0.0016 - acc: 1.0000                                      \n",
      "                                                                             \n",
      "1800/1800 [==============================]                                   \n",
      " - 0s 26us/step - loss: 0.0050 - acc: 0.9994 - val_loss: 0.0284 - val_acc: 0.9850\n",
      "\n",
      "Epoch 9/50                                                                   \n",
      " 100/1800 [>.............................]                                   \n",
      " - ETA: 0s - loss: 0.0023 - acc: 1.0000                                      \n",
      "                                                                             \n",
      "1800/1800 [==============================]                                   \n",
      " - 0s 26us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.0174 - val_acc: 0.9900\n",
      "\n",
      "Best validation acc of epoch:                                                \n",
      "1.0                                                                          \n",
      "Train on 1800 samples, validate on 200 samples                               \n",
      "Epoch 1/50                                                    \n",
      "  50/1800 [..............................]                    \n",
      " - ETA: 6s - loss: 0.7191 - acc: 0.2000                       \n",
      "                                                              \n",
      "1000/1800 [===============>..............]                    \n",
      " - ETA: 0s - loss: 0.5590 - acc: 0.7400                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 172us/step - loss: 0.4985 - acc: 0.7656 - val_loss: 0.1864 - val_acc: 1.0000\n",
      "\n",
      "Epoch 2/50                                                    \n",
      "  50/1800 [..............................]                    \n",
      " - ETA: 0s - loss: 0.3526 - acc: 0.8000                       \n",
      "                                                              \n",
      "1100/1800 [=================>............]                    \n",
      " - ETA: 0s - loss: 0.2559 - acc: 0.8818                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 53us/step - loss: 0.1990 - acc: 0.9200 - val_loss: 0.0526 - val_acc: 0.9950\n",
      "\n",
      "Epoch 3/50                                                    \n",
      "  50/1800 [..............................]                    \n",
      " - ETA: 0s - loss: 0.0774 - acc: 0.9800                       \n",
      "                                                              \n",
      "1050/1800 [================>.............]                    \n",
      " - ETA: 0s - loss: 0.0454 - acc: 0.9905                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 57us/step - loss: 0.0366 - acc: 0.9933 - val_loss: 0.0396 - val_acc: 0.9800\n",
      "\n",
      "Epoch 4/50                                                    \n",
      "  50/1800 [..............................]                    \n",
      " - ETA: 0s - loss: 0.0122 - acc: 1.0000                       \n",
      "                                                              \n",
      "1000/1800 [===============>..............]                    \n",
      " - ETA: 0s - loss: 0.0132 - acc: 0.9990                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 60us/step - loss: 0.0123 - acc: 0.9989 - val_loss: 0.0335 - val_acc: 0.9850\n",
      "\n",
      "Epoch 5/50                                                    \n",
      "  50/1800 [..............................]                    \n",
      " - ETA: 0s - loss: 0.0367 - acc: 0.9800                       \n",
      "                                                              \n",
      "1050/1800 [================>.............]                    \n",
      " - ETA: 0s - loss: 0.0066 - acc: 0.9990                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 55us/step - loss: 0.0058 - acc: 0.9994 - val_loss: 0.0292 - val_acc: 0.9850\n",
      "\n",
      "Epoch 6/50                                                    \n",
      "  50/1800 [..............................]                    \n",
      " - ETA: 0s - loss: 0.0020 - acc: 1.0000                       \n",
      "                                                              \n",
      "1000/1800 [===============>..............]                    \n",
      " - ETA: 0s - loss: 0.0026 - acc: 1.0000                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 60us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0245 - val_acc: 0.9850\n",
      "\n",
      "Epoch 7/50                                                    \n",
      "  50/1800 [..............................]                    \n",
      " - ETA: 0s - loss: 8.6571e-04 - acc: 1.0000                   \n",
      "                                                              \n",
      "1000/1800 [===============>..............]                    \n",
      " - ETA: 0s - loss: 0.0020 - acc: 1.0000                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 57us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9800\n",
      "\n",
      "Best validation acc of epoch:                                 \n",
      "1.0                                                           \n",
      "Train on 1800 samples, validate on 200 samples                \n",
      "Epoch 1/20                                                    \n",
      " 100/1800 [>.............................]                    \n",
      " - ETA: 3s - loss: 0.6882 - acc: 0.6600                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 160us/step - loss: 0.5539 - acc: 0.7856 - val_loss: 0.1584 - val_acc: 1.0000\n",
      "\n",
      "Epoch 2/20                                                    \n",
      " 100/1800 [>.............................]                    \n",
      " - ETA: 0s - loss: 0.5596 - acc: 0.7500                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 29us/step - loss: 0.4399 - acc: 0.7972 - val_loss: 0.2177 - val_acc: 1.0000\n",
      "\n",
      "Epoch 3/20                                                    \n",
      " 100/1800 [>.............................]                    \n",
      " - ETA: 0s - loss: 0.3339 - acc: 0.8600                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 31us/step - loss: 0.2878 - acc: 0.8817 - val_loss: 0.1155 - val_acc: 1.0000\n",
      "\n",
      "Epoch 4/20                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100/1800 [>.............................]                    \n",
      " - ETA: 0s - loss: 0.1804 - acc: 0.9600                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 29us/step - loss: 0.1086 - acc: 0.9761 - val_loss: 0.0539 - val_acc: 0.9950\n",
      "\n",
      "Epoch 5/20                                                    \n",
      " 100/1800 [>.............................]                    \n",
      " - ETA: 0s - loss: 0.0426 - acc: 1.0000                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 28us/step - loss: 0.0369 - acc: 0.9933 - val_loss: 0.0323 - val_acc: 0.9950\n",
      "\n",
      "Epoch 6/20                                                    \n",
      " 100/1800 [>.............................]                    \n",
      " - ETA: 0s - loss: 0.0146 - acc: 1.0000                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 31us/step - loss: 0.0171 - acc: 0.9978 - val_loss: 0.0269 - val_acc: 0.9950\n",
      "\n",
      "Epoch 7/20                                                    \n",
      " 100/1800 [>.............................]                    \n",
      " - ETA: 0s - loss: 0.0149 - acc: 1.0000                       \n",
      "                                                              \n",
      "1500/1800 [========================>.....]                    \n",
      " - ETA: 0s - loss: 0.0107 - acc: 0.9980                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 42us/step - loss: 0.0100 - acc: 0.9983 - val_loss: 0.0248 - val_acc: 1.0000\n",
      "\n",
      "Epoch 8/20                                                    \n",
      " 100/1800 [>.............................]                    \n",
      " - ETA: 0s - loss: 0.0052 - acc: 1.0000                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 30us/step - loss: 0.0063 - acc: 0.9994 - val_loss: 0.0228 - val_acc: 1.0000\n",
      "\n",
      "Epoch 9/20                                                    \n",
      " 100/1800 [>.............................]                    \n",
      " - ETA: 0s - loss: 0.0060 - acc: 1.0000                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 30us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.0237 - val_acc: 0.9950\n",
      "\n",
      "Epoch 10/20                                                   \n",
      " 100/1800 [>.............................]                    \n",
      " - ETA: 0s - loss: 0.0032 - acc: 1.0000                       \n",
      "                                                              \n",
      "1700/1800 [===========================>..]                    \n",
      " - ETA: 0s - loss: 0.0030 - acc: 1.0000                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 35us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0222 - val_acc: 0.9950\n",
      "\n",
      "Best validation acc of epoch:                                 \n",
      "1.0                                                           \n",
      "Train on 1800 samples, validate on 200 samples                \n",
      "Epoch 1/100                                                   \n",
      "  50/1800 [..............................]                    \n",
      " - ETA: 7s - loss: 0.6839 - acc: 0.6800                       \n",
      "                                                              \n",
      " 900/1800 [==============>...............]                    \n",
      " - ETA: 0s - loss: 0.4848 - acc: 0.7767                       \n",
      "                                                              \n",
      "1750/1800 [============================>.]                    \n",
      " - ETA: 0s - loss: 0.3657 - acc: 0.8389                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 208us/step - loss: 0.3585 - acc: 0.8428 - val_loss: 0.0487 - val_acc: 1.0000\n",
      "\n",
      "Epoch 2/100                                                   \n",
      "  50/1800 [..............................]                    \n",
      " - ETA: 0s - loss: 0.1405 - acc: 0.9400                       \n",
      "                                                              \n",
      "1000/1800 [===============>..............]                    \n",
      " - ETA: 0s - loss: 0.1017 - acc: 0.9730                       \n",
      "                                                              \n",
      "1700/1800 [===========================>..]                    \n",
      " - ETA: 0s - loss: 0.0825 - acc: 0.9794                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 68us/step - loss: 0.0802 - acc: 0.9794 - val_loss: 0.0502 - val_acc: 0.9800\n",
      "\n",
      "Epoch 3/100                                                   \n",
      "  50/1800 [..............................]                    \n",
      " - ETA: 0s - loss: 0.0491 - acc: 0.9800                       \n",
      "                                                              \n",
      " 900/1800 [==============>...............]                    \n",
      " - ETA: 0s - loss: 0.0358 - acc: 0.9922                       \n",
      "                                                              \n",
      "1700/1800 [===========================>..]                    \n",
      " - ETA: 0s - loss: 0.0289 - acc: 0.9935                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 68us/step - loss: 0.0278 - acc: 0.9939 - val_loss: 0.0322 - val_acc: 0.9800\n",
      "\n",
      "Epoch 4/100                                                   \n",
      "  50/1800 [..............................]                    \n",
      " - ETA: 0s - loss: 0.0296 - acc: 0.9800                       \n",
      "                                                              \n",
      " 950/1800 [==============>...............]                    \n",
      " - ETA: 0s - loss: 0.0153 - acc: 0.9947                       \n",
      "                                                              \n",
      "1600/1800 [=========================>....]                    \n",
      " - ETA: 0s - loss: 0.0152 - acc: 0.9938                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 71us/step - loss: 0.0138 - acc: 0.9944 - val_loss: 0.0278 - val_acc: 0.9900\n",
      "\n",
      "Epoch 5/100                                                   \n",
      "  50/1800 [..............................]                    \n",
      " - ETA: 0s - loss: 0.0197 - acc: 0.9800                       \n",
      "                                                              \n",
      " 900/1800 [==============>...............]                    \n",
      " - ETA: 0s - loss: 0.0080 - acc: 0.9967                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 62us/step - loss: 0.0069 - acc: 0.9972 - val_loss: 0.0092 - val_acc: 1.0000\n",
      "\n",
      "Epoch 6/100                                                   \n",
      "  50/1800 [..............................]                    \n",
      " - ETA: 0s - loss: 0.0013 - acc: 1.0000                       \n",
      "                                                              \n",
      " 950/1800 [==============>...............]                    \n",
      " - ETA: 0s - loss: 0.0031 - acc: 0.9989                       \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 59us/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0300 - val_acc: 0.9850\n",
      "\n",
      "Epoch 7/100                                                   \n",
      "  50/1800 [..............................]                    \n",
      " - ETA: 0s - loss: 7.7729e-05 - acc: 1.0000                   \n",
      "                                                              \n",
      " 850/1800 [=============>................]                    \n",
      " - ETA: 0s - loss: 0.0013 - acc: 1.0000                       \n",
      "                                                              \n",
      "1650/1800 [==========================>...]                    \n",
      " - ETA: 0s - loss: 0.0012 - acc: 1.0000                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 68us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0076 - val_acc: 1.0000\n",
      "\n",
      "Epoch 8/100                                                   \n",
      "  50/1800 [..............................]                    \n",
      " - ETA: 0s - loss: 6.4089e-04 - acc: 1.0000                   \n",
      "                                                              \n",
      " 950/1800 [==============>...............]                    \n",
      " - ETA: 0s - loss: 3.6878e-04 - acc: 1.0000                   \n",
      "                                                              \n",
      "1750/1800 [============================>.]                    \n",
      " - ETA: 0s - loss: 4.9429e-04 - acc: 1.0000                   \n",
      "                                                              \n",
      "1800/1800 [==============================]                    \n",
      " - 0s 65us/step - loss: 4.8501e-04 - acc: 1.0000 - val_loss: 0.0206 - val_acc: 0.9900\n",
      "\n",
      "Best validation acc of epoch:                                 \n",
      "1.0                                                           \n",
      "100%|██████████| 5/5 [00:09<00:00,  1.91s/it, best loss: -1.0]\n",
      "Evalutation of best performing model:\n",
      "162/162 [==============================] - 0s 33us/step\n",
      "[0.015970908383476652, 0.9938271604938271]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Conv1D': 1, 'Conv1D_1': 0, 'Conv1D_2': 0, 'batch_size': 1, 'epochs': 2, 'optimizer': 0}\n"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='Malicious_URLs_Detection')\n",
    "\n",
    "Xtrain, ytrain, Xtest, ytest = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(Xtest, ytest))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:49.259440Z",
     "start_time": "2019-08-28T11:07:48.496907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "1800/1800 [==============================] - 0s 167us/step - loss: 0.6240 - acc: 0.7956 - val_loss: 0.4551 - val_acc: 1.0000\n",
      "Epoch 2/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.5089 - acc: 0.7956 - val_loss: 0.2549 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f83203e86a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks_list = [\n",
    "    callbacks.EarlyStopping(\n",
    "    monitor='acc',\n",
    "    patience=1,),\n",
    "    callbacks.ModelCheckpoint(\n",
    "    filepath='my_model2.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,)] \n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(len(word_index) + 1, 128, input_length=maxlen))\n",
    "model.add(layers.SeparableConv1D(64, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(Xtrain, ytrain, epochs=50, batch_size=100, validation_split=0.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:49.269436Z",
     "start_time": "2019-08-28T11:07:49.260571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 33us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25583459970391825, 1.0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:49.323910Z",
     "start_time": "2019-08-28T11:07:49.270286Z"
    }
   },
   "outputs": [],
   "source": [
    "ypred = model.predict(Xtest)\n",
    "for i, n in enumerate(ypred):\n",
    "    if n > 0.5:\n",
    "        ypred[i] = 1\n",
    "    else:\n",
    "        ypred[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:49.327746Z",
     "start_time": "2019-08-28T11:07:49.324760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:49.332391Z",
     "start_time": "2019-08-28T11:07:49.328436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:49.336626Z",
     "start_time": "2019-08-28T11:07:49.333112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:50.533299Z",
     "start_time": "2019-08-28T11:07:49.337458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "1800/1800 [==============================] - 0s 174us/step - loss: 0.5230 - acc: 0.7783 - val_loss: 0.1853 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.2624 - acc: 0.8783 - val_loss: 0.0746 - val_acc: 0.9950\n",
      "Epoch 3/10\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.0546 - acc: 0.9911 - val_loss: 0.0398 - val_acc: 0.9900\n",
      "Epoch 4/10\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.0160 - acc: 0.9972 - val_loss: 0.0225 - val_acc: 0.9950\n",
      "Epoch 5/10\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.0072 - acc: 0.9989 - val_loss: 0.0202 - val_acc: 0.9950\n",
      "Epoch 6/10\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.0039 - acc: 0.9994 - val_loss: 0.0221 - val_acc: 0.9950\n",
      "Epoch 7/10\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0170 - val_acc: 0.9950\n",
      "Epoch 8/10\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0185 - val_acc: 0.9950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f830c106e10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks_list = [\n",
    "    callbacks.EarlyStopping(\n",
    "    monitor='acc',\n",
    "    patience=1,),\n",
    "    callbacks.ModelCheckpoint(\n",
    "    filepath='my_model3.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,)]\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(len(word_index) + 1, 64, input_length=maxlen))\n",
    "model.add(layers.Conv1D(64, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='nadam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(Xtrain, ytrain, epochs=10, batch_size=100, validation_split=0.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:50.676846Z",
     "start_time": "2019-08-28T11:07:50.534482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 859us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.022874159508465247, 0.9876543209876543]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:50.736825Z",
     "start_time": "2019-08-28T11:07:50.677678Z"
    }
   },
   "outputs": [],
   "source": [
    "ypred = model.predict(Xtest)\n",
    "for i, n in enumerate(ypred):\n",
    "    if n > 0.5:\n",
    "        ypred[i] = 1\n",
    "    else:\n",
    "        ypred[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:50.740621Z",
     "start_time": "2019-08-28T11:07:50.737702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:50.744688Z",
     "start_time": "2019-08-28T11:07:50.741411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9876543209876543"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:50.749137Z",
     "start_time": "2019-08-28T11:07:50.746209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9937888198757764"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:50.774883Z",
     "start_time": "2019-08-28T11:07:50.749997Z"
    }
   },
   "outputs": [],
   "source": [
    "urls_lst = []\n",
    "maxlen = 0\n",
    "for url in malicious_urls[0].values:\n",
    "    url_lst = []\n",
    "    for char in url:\n",
    "        try:\n",
    "            url_lst.append(ord(char))\n",
    "        except TypeError:\n",
    "            url_lst.append(ord('?'))\n",
    "    urls_lst.append(url_lst)\n",
    "    getlen = len(url_lst)\n",
    "    if getlen > maxlen:\n",
    "        maxlen = getlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:50.799458Z",
     "start_time": "2019-08-28T11:07:50.775967Z"
    }
   },
   "outputs": [],
   "source": [
    "X = preprocessing.sequence.pad_sequences(urls_lst, maxlen=maxlen, padding='post')\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1)) / 255.0\n",
    "y = malicious_urls[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:50.802251Z",
     "start_time": "2019-08-28T11:07:50.800383Z"
    }
   },
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = X[:2000], X[2000:], y[:2000], y[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:50.806730Z",
     "start_time": "2019-08-28T11:07:50.803010Z"
    }
   },
   "outputs": [],
   "source": [
    "class ActivationLogger(callbacks.Callback):\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        layer_output = model.layers[-1].output\n",
    "        self.activations_model = models.Model(model.input, layer_output)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is None:\n",
    "            raise RuntimeError('Requires validation_data.')\n",
    "        validation_sample = self.validation_data[0][0:1]\n",
    "        activations = self.activations_model.predict(validation_sample)\n",
    "        f = open('activations_at_epoch_' + str(epoch) + '.npz', 'wb')\n",
    "        np.savez(f, activations)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:53.213160Z",
     "start_time": "2019-08-28T11:07:50.807579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "1800/1800 [==============================] - 0s 227us/step - loss: 0.4612 - acc: 0.7678 - val_loss: 0.1584 - val_acc: 0.9900\n",
      "Epoch 2/10\n",
      "1800/1800 [==============================] - 0s 45us/step - loss: 0.3996 - acc: 0.8272 - val_loss: 0.1453 - val_acc: 0.9700\n",
      "Epoch 3/10\n",
      "1800/1800 [==============================] - 0s 84us/step - loss: 0.3724 - acc: 0.8450 - val_loss: 0.1293 - val_acc: 0.9750\n",
      "Epoch 4/10\n",
      "1800/1800 [==============================] - 0s 44us/step - loss: 0.3532 - acc: 0.8583 - val_loss: 0.3918 - val_acc: 0.8400\n",
      "Epoch 5/10\n",
      "1800/1800 [==============================] - 0s 52us/step - loss: 0.3414 - acc: 0.8639 - val_loss: 0.0974 - val_acc: 0.9750\n",
      "Epoch 6/10\n",
      "1800/1800 [==============================] - 0s 105us/step - loss: 0.3294 - acc: 0.8639 - val_loss: 0.2553 - val_acc: 0.9350\n",
      "Epoch 7/10\n",
      "1800/1800 [==============================] - 0s 107us/step - loss: 0.2994 - acc: 0.8867 - val_loss: 0.1615 - val_acc: 0.9700\n",
      "Epoch 8/10\n",
      "1800/1800 [==============================] - 0s 82us/step - loss: 0.2937 - acc: 0.8906 - val_loss: 0.1543 - val_acc: 0.9700\n",
      "Epoch 9/10\n",
      "1800/1800 [==============================] - 0s 77us/step - loss: 0.2771 - acc: 0.8900 - val_loss: 0.1741 - val_acc: 0.9700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f82e85d0710>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks_list = [\n",
    "    callbacks.EarlyStopping(\n",
    "    monitor='acc',\n",
    "    patience=1,),\n",
    "    callbacks.ModelCheckpoint(\n",
    "    filepath='my_model4.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,), ActivationLogger()] \n",
    "\n",
    "activation = ActivationLogger()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv1D(64, 7, activation='relu', kernel_initializer='lecun_normal'))\n",
    "model.add(layers.MaxPooling1D(7))\n",
    "model.add(layers.Conv1D(64, 9, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='nadam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(Xtrain, ytrain, epochs=10, batch_size=100, validation_split=0.1, callbacks=[activation] + callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:53.368313Z",
     "start_time": "2019-08-28T11:07:53.216958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 904us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17353871943038188, 0.9567901234567902]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:53.439681Z",
     "start_time": "2019-08-28T11:07:53.369249Z"
    }
   },
   "outputs": [],
   "source": [
    "ypred = model.predict(Xtest)\n",
    "for i, n in enumerate(ypred):\n",
    "    if n > 0.5:\n",
    "        ypred[i] = 1\n",
    "    else:\n",
    "        ypred[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:53.443792Z",
     "start_time": "2019-08-28T11:07:53.440603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:53.447937Z",
     "start_time": "2019-08-28T11:07:53.444578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9567901234567902"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:53.453872Z",
     "start_time": "2019-08-28T11:07:53.448646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9779179810725552"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上模型使用了两种方法，但都是基于一维卷积进行的：\n",
    "\n",
    "1. 基于一维卷积的文本序列，将所有文本都转换为整数型数据，然后再结合词嵌入进行词向量的降维得到；\n",
    "2. 思想是将每个文本序列转换为ascii码，因ascii码的范围恰遇图像的RGB和灰度范围都一致，将其看做是一个深度为1的灰度图，再使用一维卷积\n",
    "\n",
    "整体来看，使用方法的效果模型更好，速度快，模型更简单，得到的精度也很高，而方法二的思路相对新颖，但模型相比方法一较复杂，训练时间也较长，，对于两个模型的解释，主要还是在一维卷积这种对于无序小数据文本处理的优势上！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T11:07:53.577352Z",
     "start_time": "2019-08-28T11:07:53.454572Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_model(model, show_shapes=True, to_file='model.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
