{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在DNN这类全连接神经网络算法中，隐藏层的前一层的每个节点都需要与下一层的每个节点连接，当节点数量巨大时，产生的连接就非常多，这样，在硬件环境有限的情况下就几乎难以完成训练过程。\n",
    "\n",
    "为了解决图像处理领域全连接造成的计算量巨大的问题，人们提出了局部连接。其理论基础是基于这样的假设：生物在进行图像识别时，对图像的理解只需要处理局部的数据即可，不需要全面分析全部图像后才能进行处理。\n",
    "\n",
    "所谓权值共享是指当从一个大尺寸图像中随机选取一小块，比如说8×8作为样本，并且从这个小块样本中学习到了一些特征，这时我们可以把从这个8×8样本中学习到的特征作为探测器，应用到这个图像的任意地方。特别是，我们可以用从8×8样本中所学习到的特征跟原本的大尺寸图像作卷积，从而在这个大尺寸图像上的任一位置获得一个不同特征的激活值。\n",
    "\n",
    "所谓池化是指人们可以计算图像一个区域上的某个特定特征的平均值或者最大值。这些概要统计特征不仅具有低得多的维度，同时还会改善结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T07:44:23.044595Z",
     "start_time": "2018-11-25T07:43:58.795022Z"
    }
   },
   "outputs": [],
   "source": [
    "from tflearn.datasets import mnist\n",
    "from tflearn import input_data, conv_2d, residual_bottleneck, activation,\\\n",
    "batch_normalization, global_avg_pool, fully_connected, regression, DNN\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-24T07:00:01.166044Z",
     "start_time": "2018-11-24T07:00:00.400689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = mnist.load_data(one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-24T07:00:01.173306Z",
     "start_time": "2018-11-24T07:00:01.168587Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-24T07:00:03.866610Z",
     "start_time": "2018-11-24T07:00:01.176898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/johnnie/anaconda3/lib/python3.6/site-packages/tflearn/initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From /Users/johnnie/anaconda3/lib/python3.6/site-packages/tflearn/objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "net = input_data(shape=[None, 28, 28, 1])\n",
    "net = conv_2d(net, 64, 3, activation='relu', bias=False)\n",
    "net = residual_bottleneck(net, 3, 16, 64)\n",
    "net = residual_bottleneck(net, 1, 32, 128, downsample=True)\n",
    "net = residual_bottleneck(net, 2, 32, 128)\n",
    "net = residual_bottleneck(net, 1, 64, 256, downsample=True)\n",
    "net = residual_bottleneck(net, 2, 64, 256)\n",
    "net = batch_normalization(net)\n",
    "net = activation(net, 'relu')\n",
    "net = global_avg_pool(net)\n",
    "net = fully_connected(net, 10, activation='softmax')\n",
    "net = regression(net, optimizer='adam', loss='categorical_crossentropy', learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-24T10:48:53.338046Z",
     "start_time": "2018-11-24T07:00:03.868832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.17937\u001b[0m\u001b[0m | time: 2643.740s\n",
      "| Adam | epoch: 005 | loss: 0.17937 - acc: 0.9446 -- iter: 54784/55000\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.18357\u001b[0m\u001b[0m | time: 2728.319s\n",
      "| Adam | epoch: 005 | loss: 0.18357 - acc: 0.9435 | val_loss: 3.86709 - val_acc: 0.4834 -- iter: 55000/55000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model = DNN(net, checkpoint_path='model_resnet_mnist', max_checkpoints=10, tensorboard_verbose=0)\n",
    "model.fit(X_train, Y_train, n_epoch=5, validation_set=(X_test, Y_test), show_metric=True, batch_size=512, run_id='resnet_mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T12:08:58.147556Z",
     "start_time": "2018-11-25T12:08:48.749181Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, concatenate, Input, Average, Add, BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T04:30:50.869998Z",
     "start_time": "2018-11-25T04:30:50.857508Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH_NEG = './数据集/movie-review-data/review_polarity/txt_sentoken/neg/'\n",
    "PATH_POS = './数据集/movie-review-data/review_polarity/txt_sentoken/pos/'\n",
    "\n",
    "def build_vocab(path1, path2):\n",
    "    fileList1 = glob.glob(path1 + '*.txt')\n",
    "    fileList2 = glob.glob(path2 + '*.txt')\n",
    "    fileList = fileList1 + fileList2\n",
    "    counter = Counter()\n",
    "    for file in fileList:\n",
    "        f = open(file, 'r')\n",
    "        for line in f:\n",
    "            for word in nltk.word_tokenize(line.strip()):\n",
    "                if word.isdigit():\n",
    "                    word = \"9\"\n",
    "                counter[word] += 1\n",
    "    word2idx = {w:i+2 for i, w in enumerate(counter)}\n",
    "    word2idx['PAD'] = 0\n",
    "    word2idx['UNK'] = 1\n",
    "    idx2word = {v:k for k, v in word2idx.items()}\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T04:30:53.005220Z",
     "start_time": "2018-11-25T04:30:52.996659Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_maxlen(path1, path2):\n",
    "    sent_len = 0\n",
    "    sent_maxlen = 0\n",
    "    fileList1 = glob.glob(path1 + '*.txt')\n",
    "    fileList2 = glob.glob(path2 + '*.txt')\n",
    "    fileList = fileList1 + fileList2\n",
    "    for file in fileList:\n",
    "        f = open(file, 'r')\n",
    "        for line in f:\n",
    "            sent_len = len(nltk.word_tokenize(line.strip()))\n",
    "            if sent_len > sent_maxlen:\n",
    "                sent_maxlen = sent_len\n",
    "    return sent_maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T04:30:55.095341Z",
     "start_time": "2018-11-25T04:30:55.081999Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize(path1, path2, word2idx):\n",
    "    sentences = []\n",
    "    fileList1 = glob.glob(path1 + '*.txt')\n",
    "    fileList2 = glob.glob(path2 + '*.txt')\n",
    "    fileList = fileList1 + fileList2\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for file in fileList:\n",
    "        f = open(file, 'r')\n",
    "        for line in f:\n",
    "            sentence = []\n",
    "            for word in nltk.word_tokenize(line.strip()):\n",
    "                if word.isdigit():\n",
    "                    word = \"9\"\n",
    "                try:\n",
    "                    sentence.append(word2idx[word])\n",
    "                except KeyError:\n",
    "                    sentence.append(word2idx['UNK'])                    \n",
    "            sentences.append(sentence)\n",
    "            if file not in fileList2:\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "    X = pad_sequences(sentences, maxlen=get_maxlen(path1, path2))\n",
    "    Y = to_categorical(i * [0] + j * [1], num_classes=2)\n",
    "    return X, Y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T04:33:38.970787Z",
     "start_time": "2018-11-25T04:30:57.410039Z"
    }
   },
   "outputs": [],
   "source": [
    "word2idx, idx2word = build_vocab(PATH_POS, PATH_NEG)\n",
    "maxlen = get_maxlen(PATH_POS, PATH_NEG)\n",
    "X, Y = vectorize(PATH_POS, PATH_NEG, word2idx)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T04:35:58.617276Z",
     "start_time": "2018-11-25T04:35:57.946741Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_LAYER_SIZE = 64\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "inputs = Input(shape=(maxlen,))\n",
    "net = Embedding(len(word2idx), EMBEDDING_SIZE, input_length=maxlen, name='input')(inputs)\n",
    "conv1d_3 = Conv1D(HIDDEN_LAYER_SIZE, 3, padding='same', activation='relu', name='conv1d_3', kernel_regularizer=regularizers.l2(0.01))(net)\n",
    "conv1d_4 = Conv1D(HIDDEN_LAYER_SIZE, 4, padding='same', activation='relu', name='conv1d_4', kernel_regularizer=regularizers.l2(0.01))(net)\n",
    "conv1d_5 = Conv1D(HIDDEN_LAYER_SIZE, 5, padding='same', activation='relu', name='conv1d_5', kernel_regularizer=regularizers.l2(0.01))(net)\n",
    "average = Average()([conv1d_3, conv1d_4, conv1d_5])\n",
    "net = GlobalAveragePooling1D()(average)\n",
    "net = Dropout(0.5)(net)\n",
    "batch = BatchNormalization()(net)\n",
    "output = Dense(2, activation='sigmoid')(batch)\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:11:47.201235Z",
     "start_time": "2018-11-25T04:36:00.479038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45304 samples, validate on 19416 samples\n",
      "Epoch 1/10\n",
      "45304/45304 [==============================] - 271s 6ms/step - loss: 1.0814 - acc: 0.5508 - val_loss: 0.6760 - val_acc: 0.6037\n",
      "Epoch 2/10\n",
      "45304/45304 [==============================] - 230s 5ms/step - loss: 0.6397 - acc: 0.6629 - val_loss: 0.6357 - val_acc: 0.6608\n",
      "Epoch 3/10\n",
      "45304/45304 [==============================] - 210s 5ms/step - loss: 0.5606 - acc: 0.7398 - val_loss: 0.6163 - val_acc: 0.6827\n",
      "Epoch 4/10\n",
      "45304/45304 [==============================] - 205s 5ms/step - loss: 0.4961 - acc: 0.7828 - val_loss: 0.6135 - val_acc: 0.6909\n",
      "Epoch 5/10\n",
      "45304/45304 [==============================] - 206s 5ms/step - loss: 0.4498 - acc: 0.8088 - val_loss: 0.6161 - val_acc: 0.6929\n",
      "Epoch 6/10\n",
      "45304/45304 [==============================] - 206s 5ms/step - loss: 0.4142 - acc: 0.8291 - val_loss: 0.6489 - val_acc: 0.6959\n",
      "Epoch 7/10\n",
      "45304/45304 [==============================] - 205s 5ms/step - loss: 0.3815 - acc: 0.8438 - val_loss: 0.6536 - val_acc: 0.6936\n",
      "Epoch 8/10\n",
      "45304/45304 [==============================] - 203s 4ms/step - loss: 0.3600 - acc: 0.8564 - val_loss: 0.6774 - val_acc: 0.6941\n",
      "Epoch 9/10\n",
      "45304/45304 [==============================] - 203s 4ms/step - loss: 0.3396 - acc: 0.8645 - val_loss: 0.6921 - val_acc: 0.6898\n",
      "Epoch 10/10\n",
      "45304/45304 [==============================] - 205s 5ms/step - loss: 0.3271 - acc: 0.8691 - val_loss: 0.7141 - val_acc: 0.6867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3cba2a90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T12:08:58.168936Z",
     "start_time": "2018-11-25T12:08:58.151129Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocab(path1, path2):\n",
    "    fileList1 = glob.glob(path1 + '*.txt')\n",
    "    fileList2 = glob.glob(path2 + '*.txt')\n",
    "    fileList = fileList1 + fileList2\n",
    "    counter = Counter()\n",
    "    documents = []\n",
    "    for file in fileList:\n",
    "        f = open(file, 'rb')\n",
    "        for line in f.readlines():\n",
    "            line = nltk.word_tokenize(line.decode('unicode-escape').strip())\n",
    "            for word in line:\n",
    "                if word.startswith('\\\\x'):\n",
    "                    word = 'UNK'\n",
    "                if word.isdigit():\n",
    "                    word = '9'\n",
    "                counter[word] += 1\n",
    "    word2idx = {w[0]:i+2 for i, w in enumerate(counter)}\n",
    "    word2idx['PAD'] = 0\n",
    "    word2idx['UNK'] = 1\n",
    "    idx2word = {v:k for k, v in word2idx.items()}\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T12:09:00.268231Z",
     "start_time": "2018-11-25T12:09:00.259520Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_maxlen(path1, path2):\n",
    "    sent_len = 0\n",
    "    sent_maxlen = 0\n",
    "    fileList1 = glob.glob(path1 + '*.txt')\n",
    "    fileList2 = glob.glob(path2 + '*.txt')\n",
    "    fileList = fileList1 + fileList2\n",
    "    for file in fileList:\n",
    "        f = open(file, 'rb')\n",
    "        for line in f.readlines():\n",
    "            line = nltk.word_tokenize(line.decode('unicode-escape').strip())\n",
    "            for word in line:\n",
    "                if word.startswith('\\\\x'):\n",
    "                    word = 'UNK'\n",
    "                if word.isdigit():\n",
    "                    word = '9'\n",
    "            sent_len = len(line)\n",
    "            if sent_len > sent_maxlen:\n",
    "                sent_maxlen = sent_len\n",
    "    return sent_maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T12:09:01.217522Z",
     "start_time": "2018-11-25T12:09:01.207406Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize(path1, path2, word2idx):\n",
    "    sentences = []\n",
    "    fileList1 = glob.glob(path1 + '*.txt')\n",
    "    fileList2 = glob.glob(path2 + '*.txt')\n",
    "    fileList = fileList1 + fileList2\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for file in fileList:\n",
    "        f = open(file, 'rb')\n",
    "        for line in f.readlines():\n",
    "            sentence = []\n",
    "            line = nltk.word_tokenize(line.decode('unicode-escape').strip())\n",
    "            for word in line:\n",
    "                if word.startswith('\\\\x'):\n",
    "                    word = 'UNK'\n",
    "                if word.isdigit():\n",
    "                    word = '9'\n",
    "                try:\n",
    "                    sentence.append(word2idx[word])\n",
    "                except KeyError:\n",
    "                    sentence.append(word2idx['UNK']) \n",
    "            sentences.append(sentence)\n",
    "            if file not in fileList2:\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "    X = pad_sequences(sentences, maxlen=get_maxlen(path1, path2))\n",
    "    Y = to_categorical(i * [0] + j * [1], num_classes=2)\n",
    "    return X, Y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T12:10:42.765717Z",
     "start_time": "2018-11-25T12:09:02.405860Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH_HAM = './数据集/enron1/ham/'\n",
    "PATH_SPAM = './数据集/enron1/spam/'\n",
    "\n",
    "word2idx_email, idx2word_email = build_vocab(PATH_HAM, PATH_SPAM)\n",
    "maxlen_email = get_maxlen(PATH_HAM, PATH_SPAM)\n",
    "X_email, Y_email = vectorize(PATH_HAM, PATH_SPAM, word2idx_email)\n",
    "X_train_email, X_test_email, Y_train_email, Y_test_email = train_test_split(X_email, Y_email, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T12:11:52.382955Z",
     "start_time": "2018-11-25T12:11:52.379277Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T12:12:16.114616Z",
     "start_time": "2018-11-25T12:12:15.672264Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_LAYER_SIZE = 64\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "inputs = Input(shape=(maxlen_email,))\n",
    "net = Embedding(len(word2idx_email), EMBEDDING_SIZE, input_length=maxlen_email, name='input')(inputs)\n",
    "conv1d_3 = Conv1D(HIDDEN_LAYER_SIZE, 3, padding='same', activation='relu', name='conv1d_3', kernel_regularizer=regularizers.l2(0.01))(net)\n",
    "conv1d_4 = Conv1D(HIDDEN_LAYER_SIZE, 4, padding='same', activation='relu', name='conv1d_4', kernel_regularizer=regularizers.l2(0.01))(net)\n",
    "conv1d_5 = Conv1D(HIDDEN_LAYER_SIZE, 5, padding='same', activation='relu', name='conv1d_5', kernel_regularizer=regularizers.l2(0.01))(net)\n",
    "average = Average()([conv1d_3, conv1d_4, conv1d_5])\n",
    "net = GlobalAveragePooling1D()(average)\n",
    "net = Dropout(0.5)(net)\n",
    "batch = BatchNormalization()(net)\n",
    "output = Dense(2, activation='sigmoid')(batch)\n",
    "model_email = Model(inputs=inputs, outputs=output)\n",
    "model_email.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T12:12:20.355331Z",
     "start_time": "2018-11-25T12:12:17.367896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70226 samples, validate on 30098 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Tensor input_2:0, specified in either feed_devices or fetch_devices was not found in the Graph",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-276fc71f7114>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_email\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_email\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_email\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_email\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_email\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \"\"\"\n\u001b[1;32m   1470\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m           self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1425\u001b[0;31m               session._session, options_ptr, status)\n\u001b[0m\u001b[1;32m   1426\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Tensor input_2:0, specified in either feed_devices or fetch_devices was not found in the Graph"
     ]
    }
   ],
   "source": [
    "model_email.fit(X_train_email, Y_train_email, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, validation_data=(X_test_email, Y_test_email))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
